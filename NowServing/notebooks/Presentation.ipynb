{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Consumer complaints data](https://catalog.data.gov/dataset/consumer-complaint-database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import pandas as pd \n",
    "# read in data, df short for dataframe\n",
    "# change your path here \n",
    "path_to_data = \"/Users/jorjacman/Downloads/Consumer_Complaints.csv\"\n",
    "df = pd.read_csv(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1083164"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date received</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub-product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Sub-issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Company public response</th>\n",
       "      <th>Company</th>\n",
       "      <th>State</th>\n",
       "      <th>ZIP code</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Consumer consent provided?</th>\n",
       "      <th>Submitted via</th>\n",
       "      <th>Date sent to company</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Timely response?</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "      <th>Complaint ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03/12/2014</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Other mortgage</td>\n",
       "      <td>Loan modification,collection,foreclosure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M&amp;T BANK CORPORATION</td>\n",
       "      <td>MI</td>\n",
       "      <td>48382</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Referral</td>\n",
       "      <td>03/17/2014</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>759217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10/01/2016</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Incorrect information on credit report</td>\n",
       "      <td>Account status</td>\n",
       "      <td>I have outdated information on my credit repor...</td>\n",
       "      <td>Company has responded to the consumer and the ...</td>\n",
       "      <td>TRANSUNION INTERMEDIATE HOLDINGS, INC.</td>\n",
       "      <td>AL</td>\n",
       "      <td>352XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>10/05/2016</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>2141773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10/17/2016</td>\n",
       "      <td>Consumer Loan</td>\n",
       "      <td>Vehicle loan</td>\n",
       "      <td>Managing the loan or lease</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I purchased a new car on XXXX XXXX. The car de...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CITIZENS FINANCIAL GROUP, INC.</td>\n",
       "      <td>PA</td>\n",
       "      <td>177XX</td>\n",
       "      <td>Older American</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>10/20/2016</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>2163100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06/08/2014</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bankruptcy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AMERICAN EXPRESS COMPANY</td>\n",
       "      <td>ID</td>\n",
       "      <td>83854</td>\n",
       "      <td>Older American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web</td>\n",
       "      <td>06/10/2014</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>885638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>09/13/2014</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>Communication tactics</td>\n",
       "      <td>Frequent or repeated calls</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CITIBANK, N.A.</td>\n",
       "      <td>VA</td>\n",
       "      <td>23233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web</td>\n",
       "      <td>09/13/2014</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1027760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date received           Product     Sub-product  \\\n",
       "0    03/12/2014          Mortgage  Other mortgage   \n",
       "1    10/01/2016  Credit reporting             NaN   \n",
       "2    10/17/2016     Consumer Loan    Vehicle loan   \n",
       "3    06/08/2014       Credit card             NaN   \n",
       "4    09/13/2014   Debt collection     Credit card   \n",
       "\n",
       "                                      Issue                   Sub-issue  \\\n",
       "0  Loan modification,collection,foreclosure                         NaN   \n",
       "1    Incorrect information on credit report              Account status   \n",
       "2                Managing the loan or lease                         NaN   \n",
       "3                                Bankruptcy                         NaN   \n",
       "4                     Communication tactics  Frequent or repeated calls   \n",
       "\n",
       "                        Consumer complaint narrative  \\\n",
       "0                                                NaN   \n",
       "1  I have outdated information on my credit repor...   \n",
       "2  I purchased a new car on XXXX XXXX. The car de...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                             Company public response  \\\n",
       "0                                                NaN   \n",
       "1  Company has responded to the consumer and the ...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                  Company State ZIP code            Tags  \\\n",
       "0                    M&T BANK CORPORATION    MI    48382             NaN   \n",
       "1  TRANSUNION INTERMEDIATE HOLDINGS, INC.    AL    352XX             NaN   \n",
       "2          CITIZENS FINANCIAL GROUP, INC.    PA    177XX  Older American   \n",
       "3                AMERICAN EXPRESS COMPANY    ID    83854  Older American   \n",
       "4                          CITIBANK, N.A.    VA    23233             NaN   \n",
       "\n",
       "  Consumer consent provided? Submitted via Date sent to company  \\\n",
       "0                        NaN      Referral           03/17/2014   \n",
       "1           Consent provided           Web           10/05/2016   \n",
       "2           Consent provided           Web           10/20/2016   \n",
       "3                        NaN           Web           06/10/2014   \n",
       "4                        NaN           Web           09/13/2014   \n",
       "\n",
       "  Company response to consumer Timely response? Consumer disputed?  \\\n",
       "0      Closed with explanation              Yes                 No   \n",
       "1      Closed with explanation              Yes                 No   \n",
       "2      Closed with explanation              Yes                 No   \n",
       "3      Closed with explanation              Yes                Yes   \n",
       "4      Closed with explanation              Yes                Yes   \n",
       "\n",
       "   Complaint ID  \n",
       "0        759217  \n",
       "1       2141773  \n",
       "2       2163100  \n",
       "3        885638  \n",
       "4       1027760  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1083164 entries, 0 to 1083163\n",
      "Data columns (total 18 columns):\n",
      "Date received                   1083164 non-null object\n",
      "Product                         1083164 non-null object\n",
      "Sub-product                     847994 non-null object\n",
      "Issue                           1083164 non-null object\n",
      "Sub-issue                       578497 non-null object\n",
      "Consumer complaint narrative    305316 non-null object\n",
      "Company public response         347949 non-null object\n",
      "Company                         1083164 non-null object\n",
      "State                           1069333 non-null object\n",
      "ZIP code                        1064705 non-null object\n",
      "Tags                            148845 non-null object\n",
      "Consumer consent provided?      540277 non-null object\n",
      "Submitted via                   1083164 non-null object\n",
      "Date sent to company            1083164 non-null object\n",
      "Company response to consumer    1083159 non-null object\n",
      "Timely response?                1083164 non-null object\n",
      "Consumer disputed?              768549 non-null object\n",
      "Complaint ID                    1083164 non-null int64\n",
      "dtypes: int64(1), object(17)\n",
      "memory usage: 148.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# understand available data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column of interest sample \n",
    "df.sample()[\"Consumer complaint narrative\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mortgage', 260335),\n",
       " ('Debt collection', 208423),\n",
       " ('Credit reporting', 140433),\n",
       " ('Credit reporting, credit repair services, or other personal consumer reports',\n",
       "  134500),\n",
       " ('Credit card', 89191),\n",
       " ('Bank account or service', 86206),\n",
       " ('Student loan', 44917),\n",
       " ('Consumer Loan', 31605),\n",
       " ('Credit card or prepaid card', 28609),\n",
       " ('Checking or savings account', 24011),\n",
       " ('Vehicle loan or lease', 7025),\n",
       " ('Money transfer, virtual currency, or money service', 6812),\n",
       " ('Payday loan', 5546),\n",
       " ('Money transfers', 5354),\n",
       " ('Payday loan, title loan, or personal loan', 5300),\n",
       " ('Prepaid card', 3819),\n",
       " ('Other financial service', 1060),\n",
       " ('Virtual currency', 18)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class distribution \n",
    "from collections import Counter\n",
    "Counter(df[\"Product\"]).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305316"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop na in narrative\n",
    "df = df[pd.notnull(df[\"Consumer complaint narrative\"])]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge similar categories\n",
    "mapping = {\n",
    "    \"Credit card\": \"Credit card or prepaid card\",\n",
    "    \"Credit reporting\": \"Credit reporting, credit repair services, or other personal consumer reports\",\n",
    "    \"Money transfers\": \"Money transfer, virtual currency, or money service\",\n",
    "    \"Payday loan\": \"Payday loan, title loan, or personal loan\",\n",
    "    \"Virtual currency\": \"Money transfer, virtual currency, or money service\",\n",
    "    \"Prepaid card\": \"Credit card or prepaid card\"\n",
    "}\n",
    "\n",
    "df = df.replace({\"Product\": mapping})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Credit reporting, credit repair services, or other personal consumer reports',\n",
       "  91411),\n",
       " ('Debt collection', 69906),\n",
       " ('Mortgage', 46369),\n",
       " ('Credit card or prepaid card', 33653),\n",
       " ('Student loan', 18004),\n",
       " ('Bank account or service', 14887),\n",
       " ('Consumer Loan', 9474),\n",
       " ('Checking or savings account', 8005),\n",
       " ('Money transfer, virtual currency, or money service', 5306),\n",
       " ('Payday loan, title loan, or personal loan', 4462),\n",
       " ('Vehicle loan or lease', 3546),\n",
       " ('Other financial service', 293)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# updated class distribution, sorted\n",
    "Counter(df[\"Product\"]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop the vague category w/ low sample size\n",
    "df = df[df[\"Product\"] != 'Other financial service']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>I have outdated information on my credit repor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Product  \\\n",
       "1  Credit reporting, credit repair services, or o...   \n",
       "\n",
       "                        Consumer complaint narrative  \n",
       "1  I have outdated information on my credit repor...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# focus on the narrative\n",
    "df = df[[\"Product\", \"Consumer complaint narrative\"]]\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: Try using more of the features, e.g. the date. If the feature is categorical, will have to encode. See: sklearn.preprocessing.LabelEncoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# bag of words\n",
    "cv = CountVectorizer(stop_words=\"english\", \n",
    "                     max_features=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: try experimenting here with passing different parameters to the vectorizer, e.g. the ngram_range. \n",
    "\n",
    "TO DO: additional text processing, e.g. contraction expansion, stemming/lemmatization\n",
    "\n",
    "TO DO: try using TFIDF (term frequency inverse document frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = cv.fit_transform(df[\"Consumer complaint narrative\"]).toarray()\n",
    "bag_of_words_df = pd.DataFrame(bag_of_words, \n",
    "                               columns=[feature for feature in cv.get_feature_names()]).set_index(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>00</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>15</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>30</th>\n",
       "      <th>able</th>\n",
       "      <th>...</th>\n",
       "      <th>wife</th>\n",
       "      <th>work</th>\n",
       "      <th>working</th>\n",
       "      <th>writing</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>xx</th>\n",
       "      <th>xxxx</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Consumer Loan</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Product  00  10  100  15  2015  \\\n",
       "1   Credit reporting, credit repair services, or o...   0   0    0   0     0   \n",
       "2                                       Consumer Loan   0   3    0   0     0   \n",
       "7   Credit reporting, credit repair services, or o...   0   0    0   0     0   \n",
       "12                                    Debt collection   0   0    0   0     0   \n",
       "16                                    Debt collection   0   0    0   0     0   \n",
       "\n",
       "    2016  2017  30  able  ...    wife  work  working  writing  written  wrong  \\\n",
       "1      0     0   0     0  ...       0     0        0        0        0      0   \n",
       "2      0     0   0     0  ...       0     0        0        0        0      0   \n",
       "7      0     0   0     0  ...       0     0        0        1        0      0   \n",
       "12     0     0   0     0  ...       0     0        0        0        0      0   \n",
       "16     0     0   0     0  ...       0     0        0        0        1      0   \n",
       "\n",
       "    xx  xxxx  year  years  \n",
       "1    0     0     0      1  \n",
       "2    2    25     1      0  \n",
       "7    0     1     0      0  \n",
       "12   0     0     0      0  \n",
       "16   0    41     1      0  \n",
       "\n",
       "[5 rows x 501 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge with original data\n",
    "df_with_bow = pd.concat([df, bag_of_words_df], axis=1).drop(\"Consumer complaint narrative\", axis=1)\n",
    "df_with_bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student loan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['help', 'loan', 'pay', 'school', 'xxxx']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bag of words sample\n",
    "import numpy as np\n",
    "n = np.random.choice(50000)\n",
    "print(df_with_bow.iloc[n][\"Product\"])\n",
    "list(df_with_bow.columns[1:][df_with_bow.iloc[n].values[1:] > 1].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: try more classifiers, e.g. logistic regression, SVC. Be sure to standardize the data if necessary. \n",
    "\n",
    "TO DO: test different parameters; can be automated. See: grid/random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split features and target\n",
    "X = df_with_bow.drop(\"Product\", axis=1)\n",
    "y = df_with_bow[\"Product\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77836498111624008"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# overall accuracy\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76709059058341944"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "f1_score(y_test, predictions, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0.7619959228554347\n",
      "Score 0.7654075442473035\n",
      "Score 0.7663424843254508\n",
      "Score 0.765811381276321\n",
      "Score 0.7663104437977075\n",
      "Mean accuracy of 5 folds is 0.77\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "splits = 5\n",
    "kf = KFold(n_splits=splits, shuffle=True)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "accs = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    score = f1_score(y_test, predictions, average=\"weighted\")\n",
    "    accs.append(score)\n",
    "    print(\"Score {}\".format(score))\n",
    "    \n",
    "print(\"Mean accuracy of {} folds is {}\".format(splits, round(np.mean(accs), 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use all data\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mortgage'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example data point \n",
    "complaint = \"mortgage is too much\"\n",
    "x_predict = cv.transform([complaint])\n",
    "model.predict(x_predict)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Bag of Words to Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import embeddings\n",
    "embedding_dimension = 300\n",
    "word_embeddings = embeddings.glove.GloveEmbedding(\"common_crawl_840\", d_emb=embedding_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: try different word embeddings. See: word2vec and fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5393099784851074,\n",
       " 0.47308000922203064,\n",
       " 0.46004000306129456,\n",
       " -0.0021617000456899405,\n",
       " 0.9738600254058838,\n",
       " -0.5797399878501892,\n",
       " 0.26308000087738037,\n",
       " 0.35378000140190125,\n",
       " 0.4237399995326996,\n",
       " 2.2569000720977783,\n",
       " -0.4085099995136261,\n",
       " 0.010920999571681023,\n",
       " 0.0022525000385940075,\n",
       " 0.07054899632930756,\n",
       " -0.44312000274658203,\n",
       " -0.13399000465869904,\n",
       " -0.31718000769615173,\n",
       " 0.34415000677108765,\n",
       " -0.056063998490571976,\n",
       " -0.087677001953125,\n",
       " 0.08006100356578827,\n",
       " 0.15408000349998474,\n",
       " -0.1426600068807602,\n",
       " -0.2718699872493744,\n",
       " 0.003391300095245242,\n",
       " -0.6018999814987183,\n",
       " -0.6907899975776672,\n",
       " -0.6645699739456177,\n",
       " 0.6168000102043152,\n",
       " -0.25374001264572144,\n",
       " -0.2653200030326843,\n",
       " 0.003116799984127283,\n",
       " 0.1532599925994873,\n",
       " 0.07588300108909607,\n",
       " 0.05749199911952019,\n",
       " -0.21755999326705933,\n",
       " 0.4061500132083893,\n",
       " -0.17818999290466309,\n",
       " -0.2797499895095825,\n",
       " -0.09681499749422073,\n",
       " -0.25843000411987305,\n",
       " -0.33754000067710876,\n",
       " -0.4851300120353699,\n",
       " -0.3213599920272827,\n",
       " -0.09493499994277954,\n",
       " 0.3135699927806854,\n",
       " -0.1564200073480606,\n",
       " 0.4169600009918213,\n",
       " 0.24709999561309814,\n",
       " 0.12861000001430511,\n",
       " -0.26805999875068665,\n",
       " -0.03315600007772446,\n",
       " 0.13142000138759613,\n",
       " -0.26309001445770264,\n",
       " 0.09805499762296677,\n",
       " -0.29475000500679016,\n",
       " 0.25933998823165894,\n",
       " 0.4122200012207031,\n",
       " -0.1448799967765808,\n",
       " 0.004678899887949228,\n",
       " 0.6371200084686279,\n",
       " 0.09632600098848343,\n",
       " -0.4953399896621704,\n",
       " -0.13370999693870544,\n",
       " 0.17013999819755554,\n",
       " 0.2936500012874603,\n",
       " -0.20252999663352966,\n",
       " -0.6403800249099731,\n",
       " -0.4820300042629242,\n",
       " 0.16559000313282013,\n",
       " 0.2713499963283539,\n",
       " 0.2711600065231323,\n",
       " -0.02865999937057495,\n",
       " -0.10627999901771545,\n",
       " -0.5918599963188171,\n",
       " 0.3474000096321106,\n",
       " 0.15490999817848206,\n",
       " 0.17709000408649445,\n",
       " 0.33425000309944153,\n",
       " 0.14733000099658966,\n",
       " 0.11748000234365463,\n",
       " 0.2990399897098541,\n",
       " -0.22675999999046326,\n",
       " -0.5374299883842468,\n",
       " -0.10419999808073044,\n",
       " 0.5727800130844116,\n",
       " -0.9085299968719482,\n",
       " 0.23425999283790588,\n",
       " -0.06804800033569336,\n",
       " -0.2578499913215637,\n",
       " 0.32927000522613525,\n",
       " 0.006738999858498573,\n",
       " -0.04664900153875351,\n",
       " -0.14746999740600586,\n",
       " 0.1491599977016449,\n",
       " -0.14147000014781952,\n",
       " -0.28731000423431396,\n",
       " 0.0036629000678658485,\n",
       " -0.1425500065088272,\n",
       " 0.27628999948501587,\n",
       " -0.22047999501228333,\n",
       " 0.31817999482154846,\n",
       " -0.3104200065135956,\n",
       " -0.4817799925804138,\n",
       " 0.130280002951622,\n",
       " 1.7365000247955322,\n",
       " 0.5749099850654602,\n",
       " -0.13241000473499298,\n",
       " -0.4459899961948395,\n",
       " -0.15293000638484955,\n",
       " -0.07821500301361084,\n",
       " -0.42329999804496765,\n",
       " 0.24601000547409058,\n",
       " 0.3700999915599823,\n",
       " -0.6882799863815308,\n",
       " 0.34852999448776245,\n",
       " 0.24180999398231506,\n",
       " -0.14146000146865845,\n",
       " 0.1266999989748001,\n",
       " -0.11691000312566757,\n",
       " 0.21310000121593475,\n",
       " -0.026265999302268028,\n",
       " 0.6242600083351135,\n",
       " -0.018758000805974007,\n",
       " -0.2866699993610382,\n",
       " -0.04270999878644943,\n",
       " -0.4458700120449066,\n",
       " 0.03742599859833717,\n",
       " -0.002291700104251504,\n",
       " 0.08988600224256516,\n",
       " 0.19193999469280243,\n",
       " -0.13402999937534332,\n",
       " -0.1796800047159195,\n",
       " 0.15929000079631805,\n",
       " -0.15929000079631805,\n",
       " -0.11855000257492065,\n",
       " -0.38082000613212585,\n",
       " 0.2435699999332428,\n",
       " 0.35806000232696533,\n",
       " -0.16832999885082245,\n",
       " 0.20377999544143677,\n",
       " 0.2835499942302704,\n",
       " 0.5071899890899658,\n",
       " 0.4940299987792969,\n",
       " -0.3237699866294861,\n",
       " 0.12999999523162842,\n",
       " -0.05999099835753441,\n",
       " -0.2040800005197525,\n",
       " 0.4469600021839142,\n",
       " -0.12319999933242798,\n",
       " 0.2365799993276596,\n",
       " -0.08812300115823746,\n",
       " -0.27524998784065247,\n",
       " -0.6077100038528442,\n",
       " -0.133650004863739,\n",
       " -0.15008999407291412,\n",
       " 0.2726399898529053,\n",
       " 0.44968000054359436,\n",
       " 0.13167999684810638,\n",
       " 0.03751799836754799,\n",
       " 0.5099599957466125,\n",
       " 0.16729000210762024,\n",
       " -0.09211800247430801,\n",
       " -0.1781100034713745,\n",
       " 0.4195699989795685,\n",
       " 0.19979000091552734,\n",
       " 0.2786099910736084,\n",
       " 0.3211599886417389,\n",
       " 0.34292998909950256,\n",
       " -0.030748000368475914,\n",
       " -0.3052299916744232,\n",
       " 0.17670999467372894,\n",
       " -0.7666400074958801,\n",
       " 0.34018999338150024,\n",
       " -0.27768999338150024,\n",
       " 0.20898999273777008,\n",
       " -0.05091100186109543,\n",
       " -0.1429000049829483,\n",
       " -0.38449999690055847,\n",
       " -0.2254199981689453,\n",
       " -0.07356200367212296,\n",
       " -0.0665770024061203,\n",
       " -0.2497200071811676,\n",
       " -0.24527999758720398,\n",
       " 0.5665799975395203,\n",
       " 0.5686399936676025,\n",
       " 0.19329999387264252,\n",
       " -0.007387199904769659,\n",
       " 0.16106000542640686,\n",
       " 0.3139300048351288,\n",
       " 0.2739099860191345,\n",
       " 0.22078000009059906,\n",
       " -0.6307500004768372,\n",
       " 0.292279988527298,\n",
       " -0.02332100085914135,\n",
       " -0.41422998905181885,\n",
       " -0.43004998564720154,\n",
       " 0.6219000220298767,\n",
       " -0.6351100206375122,\n",
       " 0.46285000443458557,\n",
       " -0.35554999113082886,\n",
       " -0.381740003824234,\n",
       " -0.1331000030040741,\n",
       " -0.15557999908924103,\n",
       " -0.20090000331401825,\n",
       " 0.09492900222539902,\n",
       " 0.2787500023841858,\n",
       " 0.10907000303268433,\n",
       " 0.05082999914884567,\n",
       " 0.6536399722099304,\n",
       " 0.08541200309991837,\n",
       " -0.2683899998664856,\n",
       " 0.46035000681877136,\n",
       " 0.5855100154876709,\n",
       " -0.0539419986307621,\n",
       " 0.42649000883102417,\n",
       " 0.7043499946594238,\n",
       " -0.040692999958992004,\n",
       " 0.123089998960495,\n",
       " -0.5285199880599976,\n",
       " 0.17102999985218048,\n",
       " -0.29537999629974365,\n",
       " -0.4424799978733063,\n",
       " -0.24884000420570374,\n",
       " 0.09255900233983994,\n",
       " 0.10388000309467316,\n",
       " 0.02555900067090988,\n",
       " 0.5700600147247314,\n",
       " 0.03261199966073036,\n",
       " -0.18129999935626984,\n",
       " 0.15528999269008636,\n",
       " -0.04346499964594841,\n",
       " -0.13169999420642853,\n",
       " 0.1687300056219101,\n",
       " 0.04912099987268448,\n",
       " 0.08473300188779831,\n",
       " 0.04519300162792206,\n",
       " -0.24661999940872192,\n",
       " 0.11663000285625458,\n",
       " 0.1846799999475479,\n",
       " 0.5389800071716309,\n",
       " -0.37553998827934265,\n",
       " 0.13158999383449554,\n",
       " 0.15519000589847565,\n",
       " -0.46417000889778137,\n",
       " 0.14709000289440155,\n",
       " 0.18231000006198883,\n",
       " 0.004826600197702646,\n",
       " 0.19091999530792236,\n",
       " 0.24289999902248383,\n",
       " -0.3208700120449066,\n",
       " 0.18574999272823334,\n",
       " -0.26069000363349915,\n",
       " -0.004425100050866604,\n",
       " 0.16389000415802002,\n",
       " -0.06470700353384018,\n",
       " 0.7107200026512146,\n",
       " -0.2741999924182892,\n",
       " 0.3719800114631653,\n",
       " -0.020871000364422798,\n",
       " 0.22856000065803528,\n",
       " -0.4267300069332123,\n",
       " -0.16946999728679657,\n",
       " -0.2919299900531769,\n",
       " 0.2923099994659424,\n",
       " 0.23270000517368317,\n",
       " -0.2569099962711334,\n",
       " 0.0949229970574379,\n",
       " -0.8617200255393982,\n",
       " 0.06644699722528458,\n",
       " -0.8333100080490112,\n",
       " -0.17332999408245087,\n",
       " -0.1147100031375885,\n",
       " 0.13389000296592712,\n",
       " 0.382889986038208,\n",
       " 0.31279999017715454,\n",
       " 0.1282700002193451,\n",
       " -0.23652000725269318,\n",
       " -0.3085300028324127,\n",
       " 0.09902100265026093,\n",
       " 0.22280000150203705,\n",
       " -0.08774399757385254,\n",
       " 0.29826000332832336,\n",
       " 0.2605400085449219,\n",
       " 0.13434000313282013,\n",
       " -0.28723999857902527,\n",
       " -0.2831200063228607,\n",
       " -0.18337999284267426,\n",
       " 0.2150699943304062,\n",
       " -0.11767999827861786,\n",
       " 0.08618400245904922,\n",
       " -0.25029000639915466,\n",
       " 0.17213000357151031,\n",
       " -0.15883000195026398,\n",
       " 0.1813499927520752,\n",
       " 0.12644000351428986,\n",
       " -0.04677300155162811,\n",
       " 0.08530999720096588,\n",
       " 0.24462999403476715,\n",
       " 0.46265000104904175]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.emb(\"Orlando\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def word_similarity(word_1, word_2): \n",
    "    \n",
    "    vec_1 = np.array(word_embeddings.emb(word_1)).reshape(1, -1)\n",
    "    vec_2 = np.array(word_embeddings.emb(word_2)).reshape(1, -1)\n",
    "    \n",
    "    return cosine_similarity(vec_1, vec_2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80168550555947127"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity(\"dog\", \"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85852143512896784"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity(\"dog\", \"puppy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3361896910630251"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity(\"dog\", \"hotdog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76157662595540798"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity(\"cool\", \"awesome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56441859204981282"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity(\"cool\", \"cold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "lucas_phone_text_messages = {\n",
    "    \"positive\": [\n",
    "        \"i love you\",\n",
    "        \"you are great\",\n",
    "        \"wonderful\",\n",
    "        \"terrific\", \n",
    "        \"ok awesome see you\",\n",
    "        \"super fun\"\n",
    "    ],\n",
    "    \"negative\": [\n",
    "        \"hate you\",\n",
    "        \"you are mean\", \n",
    "        \"awful person\",\n",
    "        \"asshole\", \n",
    "        \"you're a piece of shit\",\n",
    "        \"not cool\"\n",
    "    ],\n",
    "    \"neutral\": [\n",
    "        \"pizza?\",\n",
    "        \"what's up\",\n",
    "        \"ok see you then\", \n",
    "        \"donuts in the office\",\n",
    "        \"on my way\",\n",
    "        \"where are you\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_length = 300\n",
    "def word_to_embedding(word):\n",
    "        \n",
    "    # don't use only 0's for training\n",
    "    return word_embeddings.emb(word) if word in word_embeddings else np.zeros(embedding_vector_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_allowable_length = 15\n",
    "\n",
    "\n",
    "from nltk import word_tokenize\n",
    "def words_to_matrix(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    matrix = np.zeros((max_allowable_length, embedding_vector_length))\n",
    "    for i in range(min(max_allowable_length, len(tokens))):\n",
    "        matrix[i] = word_to_embedding(tokens[i])\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: try different tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25233001,  0.10176   , -0.67484999, ...,  0.17869   ,\n",
       "        -0.51916999,  0.33590999],\n",
       "       [ 0.16718   ,  0.30592999, -0.13682   , ..., -0.035268  ,\n",
       "         0.12809999,  0.023683  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_matrix(\"hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_training_data(training_data):\n",
    "    labels = training_data.keys()\n",
    "    label_idx = dict(zip(labels, range(len(labels))))\n",
    "\n",
    "    # tokenize the words, and determine the word length\n",
    "    training_examples = []\n",
    "    indices = []\n",
    "    for label in labels:\n",
    "        for example in training_data[label]:\n",
    "            label_encoding = [0] * len(labels)\n",
    "            label_encoding[label_idx[label]] = 1\n",
    "            indices.append(label_encoding)\n",
    "            training_examples.append(word_tokenize(example))\n",
    "\n",
    "    # store embedded vectors\n",
    "    training_data_embedded = np.zeros(shape=(len(training_examples), max_allowable_length, embedding_vector_length))\n",
    "    for i in range(len(training_examples)):\n",
    "        for j in range(min(max_allowable_length, len(training_examples[i]))):\n",
    "            training_data_embedded[i, j] = word_to_embedding(training_examples[i][j])\n",
    "    indices = np.array(indices, dtype=np.int)\n",
    "\n",
    "    return labels, training_data_embedded, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['positive', 'negative', 'neutral']),\n",
       " array([[[ 0.18732999,  0.40595001, -0.51174003, ...,  0.16495   ,\n",
       "           0.18757001,  0.53873998],\n",
       "         [ 0.13948999,  0.53452998, -0.25246999, ..., -0.015228  ,\n",
       "           0.088408  ,  0.30217001],\n",
       "         [-0.11076   ,  0.30785999, -0.51980001, ..., -0.059105  ,\n",
       "           0.47604001,  0.05661   ],\n",
       "         ..., \n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        [[-0.11076   ,  0.30785999, -0.51980001, ..., -0.059105  ,\n",
       "           0.47604001,  0.05661   ],\n",
       "         [-0.19859   , -0.062818  , -0.36614001, ..., -0.58451003,\n",
       "           0.27879   , -0.26205   ],\n",
       "         [-0.093846  ,  0.58296001, -0.019271  , ..., -0.098804  ,\n",
       "           0.027538  ,  0.30041999],\n",
       "         ..., \n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.1079    ,  0.48506999, -0.25341001, ..., -0.10304   ,\n",
       "          -0.13481   ,  0.30559   ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         ..., \n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        ..., \n",
       "        [[ 0.37492999, -0.21859001,  0.065359  , ..., -0.50312001,\n",
       "           0.018339  ,  0.77991998],\n",
       "         [ 0.089187  ,  0.25792   ,  0.26282001, ...,  0.14421   ,\n",
       "          -0.169     ,  0.26501   ],\n",
       "         [ 0.27204001, -0.06203   , -0.1884    , ...,  0.13015001,\n",
       "          -0.18317001,  0.1323    ],\n",
       "         ..., \n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        [[-0.070186  ,  0.15274   , -0.33085999, ..., -0.13734999,\n",
       "           0.1575    ,  0.61553001],\n",
       "         [ 0.08649   ,  0.14503001, -0.49020001, ...,  0.25909001,\n",
       "          -0.18599001,  0.0085633 ],\n",
       "         [-0.14031   ,  0.21403   , -0.35163   , ...,  0.0404    ,\n",
       "           0.054923  ,  0.18207   ],\n",
       "         ..., \n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]],\n",
       " \n",
       "        [[ 0.44746   ,  0.18366   , -0.33118001, ...,  0.051201  ,\n",
       "           0.10705   , -0.0021492 ],\n",
       "         [-0.19859   , -0.062818  , -0.36614001, ..., -0.58451003,\n",
       "           0.27879   , -0.26205   ],\n",
       "         [-0.11076   ,  0.30785999, -0.51980001, ..., -0.059105  ,\n",
       "           0.47604001,  0.05661   ],\n",
       "         ..., \n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "           0.        ,  0.        ]]]),\n",
       " array([[1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1]]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_training_data(lucas_phone_text_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CNN for text classification resource](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 1.0814 - acc: 0.3333\n",
      "Epoch 2/5\n",
      "18/18 [==============================] - 0s 150us/step - loss: 0.7352 - acc: 0.9444\n",
      "Epoch 3/5\n",
      "18/18 [==============================] - 0s 149us/step - loss: 0.5749 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "18/18 [==============================] - 0s 149us/step - loss: 0.4724 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "18/18 [==============================] - 0s 144us/step - loss: 0.3982 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x124f34cc0>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, training_data_embedded, indices = prep_training_data(lucas_phone_text_messages)\n",
    "\n",
    "# build model\n",
    "model = Sequential()\n",
    "num_filters = 100\n",
    "n_gram = 1\n",
    "# conv layer\n",
    "model.add(Conv1D(filters=num_filters,\n",
    "                        kernel_size=n_gram,\n",
    "                        padding=\"valid\",\n",
    "                        activation=\"relu\",\n",
    "                        input_shape=(max_allowable_length, embedding_vector_length)))\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=max_allowable_length - n_gram + 1)) # down sampling \n",
    "model.add(Flatten()) # correct dimension \n",
    "model.add(Dense(len(labels), activation=\"softmax\")) # fully connected\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "\n",
    "# train\n",
    "num_epochs = 5\n",
    "model.fit(training_data_embedded, indices, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: try different number of filters, kernel sizes, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0.21429136, 'neutral': 0.20057806, 'positive': 0.58513051}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"amazing\"\n",
    "# prep text\n",
    "matrix = np.array([words_to_matrix(text)])\n",
    "\n",
    "# make predictions\n",
    "predictions = model.predict(matrix)\n",
    "\n",
    "# format score\n",
    "predictions_and_scores = {}\n",
    "for idx, label in zip(range(len(labels)), labels):\n",
    "    predictions_and_scores[label] = predictions[0][idx]\n",
    "\n",
    "predictions_and_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Convolution1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "import embeddings\n",
    "\n",
    "class CNNEmbeddedVecClassifier:\n",
    "    def __init__(self,\n",
    "                 n_gram=1,\n",
    "                 embedding_vector_length=300,\n",
    "                 num_filters=100,\n",
    "                 max_allowable_length=15):\n",
    "        self.n_gram = n_gram\n",
    "        self.embedding_vector_length = embedding_vector_length\n",
    "        self.num_filters = num_filters\n",
    "        self.max_allowable_length = max_allowable_length\n",
    "\n",
    "        self.labels = None\n",
    "        self.model = None\n",
    "\n",
    "    def word_to_embedding(self, word):\n",
    "        \n",
    "\n",
    "        return word_embeddings.emb(word) if word in word_embeddings else np.zeros(self.embedding_vector_length)\n",
    "\n",
    "    def words_to_matrix(self, text):\n",
    "        tokens = word_tokenize(text)\n",
    "        matrix = np.zeros((self.max_allowable_length, self.embedding_vector_length))\n",
    "        for i in range(min(self.max_allowable_length, len(tokens))):\n",
    "            matrix[i] = self.word_to_embedding(tokens[i])\n",
    "        \n",
    "        return matrix\n",
    "\n",
    "    def prep_training_data(self, training_data):\n",
    "        labels = training_data.keys()\n",
    "        label_idx = dict(zip(labels, range(len(labels))))\n",
    "\n",
    "        # tokenize the words and encode the labels\n",
    "        training_examples = []\n",
    "        indices = []\n",
    "        for label in labels:\n",
    "            for example in training_data[label]:\n",
    "                label_encoding = [0] * len(labels)\n",
    "                label_encoding[label_idx[label]] = 1\n",
    "                indices.append(label_encoding)\n",
    "                training_examples.append(word_tokenize(example))\n",
    "        indices = np.array(indices, dtype=np.int)\n",
    "\n",
    "        # create embedded vectors\n",
    "        training_data_embedded = np.zeros(shape=(len(training_examples), self.max_allowable_length, self.embedding_vector_length))\n",
    "        for i in range(len(training_examples)):\n",
    "            for j in range(min(self.max_allowable_length, len(training_examples[i]))):\n",
    "                training_data_embedded[i, j] = self.word_to_embedding(training_examples[i][j])\n",
    "\n",
    "        return labels, training_data_embedded, indices\n",
    "\n",
    "    def train(self, training_data):\n",
    "        \n",
    "        # convert training data\n",
    "        self.labels, training_data_embedded, indices = self.prep_training_data(training_data)\n",
    "\n",
    "        # build model\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters=self.num_filters,\n",
    "                                kernel_size=self.n_gram,\n",
    "                                padding='valid',\n",
    "                                activation='relu',\n",
    "                                input_shape=(self.max_allowable_length, self.embedding_vector_length)))\n",
    "        model.add(MaxPooling1D(pool_size=self.max_allowable_length - self.n_gram + 1))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(len(self.labels), activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=[\"accuracy\"])\n",
    "\n",
    "        # train\n",
    "        model.fit(training_data_embedded, indices, epochs=5)\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, text):\n",
    "\n",
    "        # prep text\n",
    "        matrix = np.array([self.words_to_matrix(text)])\n",
    "\n",
    "        # make predictions\n",
    "        predictions = self.model.predict(matrix)\n",
    "\n",
    "        # format score\n",
    "        predictions_and_scores = {}\n",
    "        for idx, label in zip(range(len(self.labels)), self.labels):\n",
    "            predictions_and_scores[label] = predictions[0][idx]\n",
    "\n",
    "        return predictions_and_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': ['hate you',\n",
      "              'you are mean',\n",
      "              'awful person',\n",
      "              'asshole',\n",
      "              \"you're a piece of shit\",\n",
      "              'not cool'],\n",
      " 'neutral': ['pizza?',\n",
      "             \"what's up\",\n",
      "             'ok see you then',\n",
      "             'donuts in the office',\n",
      "             'on my way',\n",
      "             'where are you'],\n",
      " 'positive': ['i love you',\n",
      "              'you are great',\n",
      "              'wonderful',\n",
      "              'terrific',\n",
      "              'ok awesome see you',\n",
      "              'super fun']}\n"
     ]
    }
   ],
   "source": [
    "pprint(lucas_phone_text_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNEmbeddedVecClassifier(n_gram=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 1.3470 - acc: 0.2778\n",
      "Epoch 2/5\n",
      "18/18 [==============================] - 0s 175us/step - loss: 0.7595 - acc: 0.9444\n",
      "Epoch 3/5\n",
      "18/18 [==============================] - 0s 185us/step - loss: 0.5255 - acc: 1.0000\n",
      "Epoch 4/5\n",
      "18/18 [==============================] - 0s 176us/step - loss: 0.3928 - acc: 1.0000\n",
      "Epoch 5/5\n",
      "18/18 [==============================] - 0s 198us/step - loss: 0.3054 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.train(training_data=lucas_phone_text_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 0.19678415, 'neutral': 0.2164792, 'positive': 0.58673668}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"sweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
